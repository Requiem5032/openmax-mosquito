{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from utils.data_loader import create_dataset\n",
    "from utils.openmax import get_activations, compute_openmax\n",
    "from utils.openmax_utils import get_openmax_predict, get_openmax_label, convert_to_binary_label, convert_to_label, compute_roc, plot_roc, compute_pr, plot_pr\n",
    "from utils.graph import draw_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Num GPUs Available: ', len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "IMG_DIM = 299\n",
    "BATCH_SIZE = 16\n",
    "FOLD = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_CSV = f'data/datasplit_fold{FOLD}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(f'models/fold_{FOLD}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unknown_ds = create_dataset(df, BATCH_SIZE, 'test_unknown')\n",
    "_, y_test_unknown = test_unknown_ds.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_test, softmax_test = get_activations(test_unknown_ds, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openmax = []\n",
    "prob_u = []\n",
    "for logits in logits_test:\n",
    "    temp_openmax, temp_prob_u = compute_openmax(logits.reshape(1, -1), FOLD)\n",
    "    openmax.append(temp_openmax)\n",
    "    prob_u.append(temp_prob_u)\n",
    "\n",
    "openmax = np.array(openmax)\n",
    "prob_u = np.array(prob_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_unknown_binary = convert_to_binary_label(y_test_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc = compute_roc(y_test_unknown_binary, prob_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = compute_pr(y_test_unknown_binary, prob_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pr(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_thresholds = roc['thresholds']\n",
    "np.savetxt('roc_thresholds.csv', roc_thresholds, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_thresholds = pr['thresholds']\n",
    "np.savetxt('pr_thresholds.csv', pr_thresholds, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_unknown_label = convert_to_label(y_test_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_arr = []\n",
    "recall_arr = []\n",
    "f1_arr = []\n",
    "accuracy_arr = []\n",
    "for threshold in pr_thresholds:\n",
    "    y_hat_test = []\n",
    "    for probs in openmax:\n",
    "        temp = get_openmax_label(probs, threshold=threshold)\n",
    "        y_hat_test.append(temp)\n",
    "    y_hat_test = np.asarray(y_hat_test)\n",
    "\n",
    "    precision = precision_score(y_test_unknown_label, y_hat_test, average='macro')\n",
    "    recall = recall_score(y_test_unknown_label, y_hat_test, average='macro')\n",
    "    f1 = f1_score(y_test_unknown_label, y_hat_test, average='macro')\n",
    "    accuracy = accuracy_score(y_test_unknown_label, y_hat_test)\n",
    "\n",
    "    precision_arr.append(precision)\n",
    "    recall_arr.append(recall)\n",
    "    f1_arr.append(f1)\n",
    "    accuracy_arr.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = np.argmax(f1_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_arr[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_arr[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_arr[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_arr[best_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ds = create_dataset(df, BATCH_SIZE, 'test')\n",
    "# x_test, y_test = test_ds.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     random_char = np.random.randint(0, len(x_test))\n",
    "\n",
    "#     test_x1 = x_test[random_char]\n",
    "#     test_y1 = y_test[random_char]\n",
    "\n",
    "#     logits, softmax = get_activations(\n",
    "#         test_x1.reshape(-1, IMG_DIM, IMG_DIM, 3), model)\n",
    "\n",
    "#     openmax, _ = compute_openmax(logits, FOLD)\n",
    "\n",
    "#     print(f'SoftMax Sum: {np.sum(softmax)}')\n",
    "#     print(f'OpenMax Sum: {np.sum(openmax)}')\n",
    "#     print(f'True Label: {np.argmax(test_y1)}')\n",
    "#     print(f'SoftMax Label: {np.argmax(softmax)}')\n",
    "#     print(f'OpenMax Label: {get_openmax_label(openmax, pr_thresholds[best_idx])}')\n",
    "#     plt.imshow((test_x1+1)/2)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat = np.argmax(y_hat, axis=1).astype(np.uint8)\n",
    "# y_test = np.argmax(y_test, axis=1).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(y_test, y_hat, normalize='true')\n",
    "# cm = np.round(cm, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_name = range(test_ds.get_class_num())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_confusion_matrix(cm, class_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
